---
title: "生成式學習的兩種策略：要各個擊破，還是要一次到位"
date: 2026-01-24  # 文章发布时间
categories: [机器学习] # 你的分类
tags: [笔记]     # 你的标签
math: true
---

生成式AI---生成有结构的复杂物件，如文句、影像、语音

## 策略1:各个击破

![截屏2026-01-24 19.57.57](https://cdn.jsdelivr.net/gh/HEYWEEN/images@main/images%E6%88%AA%E5%B1%8F2026-01-24%2019.57.57.png)

Autoregressive(AR) Model

产生代表结束的符号时停下来

## 策略2:一次到位

![截屏2026-01-24 19.59.14](https://cdn.jsdelivr.net/gh/HEYWEEN/images@main/images%E6%88%AA%E5%B1%8F2026-01-24%2019.59.14.png)

Non-autoregressive(NAR) Model

怎么知道结束？

法一：固定输出长度，碰到[END]，后面直接丢掉

法二：先输出一个数字，再生成这么多个字

![截屏2026-01-24 20.00.57](https://cdn.jsdelivr.net/gh/HEYWEEN/images@main/images%E6%88%AA%E5%B1%8F2026-01-24%2020.00.57.png)

各个击破像是“串行化”，一次到位像是“并行化”

各个击破的生成品质通常比较好，因为在产生下一个token的时候会受到上一个token的影响，可能会产生一些看起来很奇怪的答案

## 语音合成领域

![截屏2026-01-24 20.04.33](https://cdn.jsdelivr.net/gh/HEYWEEN/images@main/images%E6%88%AA%E5%B1%8F2026-01-24%2020.04.33.png)

也可以使用“N次到位”，也就是多次反复进行一次到位的操作，比如先产生模糊的图片，然后再越来越清楚

Diffusion Model